---
title: "Which databases"
author: "Thomas Delcey"
date: "09/02/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(dplyr)
```

## Presentation of main databases available 

It exists a variety of large-scale bibliometric databases like Web of Science, Dimensions, Econlit, Proquest, Scopus, JSTOR. The main pros of such databases is that they are professionally managed by librarians. It results that the data are cleanner, the companies offers online library for easily exploring and exporting large-scale data. However, such database has not been built for historians, and especially historians of economics. Hence, for most of these databases, data become much poorer as we go back into the XX<sup>th</sup> century. 

## Performance for older periods 

For instance, for exploration purpose, you might be interested to know the number of articles mentioning a particular notion or concept, at a given period. Such result may diverge greatly from the database chosen. As an illustration, I plot below the result of a search for "rational expectations" between 1955 and 1971.

``` {r message = FALSE, echo = FALSE, warning = FALSE, out.width = '40%'}
#data
df <- readxl::read_xlsx("~/MEGA/github/learning_bibliometrics/cleaning_scripts/data_google_scholar/rational_expectations.xlsx", col_names = TRUE)

ggplot(df, aes(x=database, y = value)) + 
  geom_histogram(stat = 'identity',
                 binwidth = 1) + 
  geom_text(aes(label = value, vjust=-0.5)) 
```

JSTOR and Proquest are by far the more comprehensive databases for period pre-1990. WOS, Scopus and Econlit performed poorly for older periods, resulting in a high number of false negative. Note, however, that it does not imply that such databases are useless. We will discuss in a next post the pros and cons of each database. 

## Performance for recent period


## The case of google scholar 

Let's include now google scholar in our research of 'rational expectations'

``` {r echo = FALSE, warning = FALSE, out.width = '40%'}
#data
df <- readxl::read_xlsx("~/MEGA/github/learning_bibliometrics/cleaning_scripts/data_google_scholar/rational_expectations2.xlsx", col_names = TRUE)

ggplot(df, aes(x=database, y = value)) + 
  geom_histogram(stat = 'identity',
                 binwidth = 1) + 
  geom_text(aes(label = value, vjust=-0.5))
```

Google scholar is able to find a greater number of documents. It seems to perform far better than any database, including Proquest and Scopus. To understand why, we must understand what exactly google scholar does.

**What is google scholar ? **

Contrary to most of other services, google scholar does not index journals but individual documents. It used a web crawler, called 'parsers', to identify, in any language, 'scholarly' documents (articles, book, preprint ...). Google scholar identify a text (html or .pdf) on the web as an academic document if it detects bibliometric metadata within the text. Parsers read both the information of website (identifying the html tags and CSS classes) and metadata of the first page of the pdf it found. Parsers is also able to detect the references section of a text and collect metadata of cited references. In addition to searching on google academic documents, google scholar also include the google book database (used for the ngram device), and the US court opinions and patents. Google scholar does not provide any publicly indexed aggregated database (we discuss below the different way of exporting google scholar metadata). Google scholar provide for each documents the list of documents citing it (see [details](https://scholar.google.com/intl/en/scholar/inclusion.html#indexing)). 

Although google scholar is primarily designed for detecting published academic papers, it generally performs better to detect unusual academic documents than others database, such as preprints, books, speech, reports. Google scholar's ability to extract metadata information may be powerful tools for historians of economics to retrieve documents that have fallen into oblivion. It is generally a fast tool that overlaps a large part of the other databases. 

No matter powerful the tool is, it has also serious limitations. An unsupervised automated extraction on the web of this scale is doomed to make many mistakes. The quality of parsers extraction is obviously constrained by the quality of metadata it extracts. If html tags or first-pages from pdf are badly written, google scholar will not identify, or will badly identify, an academic documents. Symmetrically, google scholar includes also a high percentage of false positive, namely documents found by the inquiry that *are not* real documents, or documents mentioning your keyword research. Specifically, errors include, duplicated documents, non academic documents, and multiple various errors of indexing. Here is a subset of duplicates metadata from the research "rational expectations". 

``` {r echo = FALSE, warning = FALSE, out.width = '40%'}
#data
GS <- read.csv("~/MEGA/github/learning_bibliometrics/cleaning_scripts/data_google_scholar/google_scholar.csv", header = TRUE)

GS_duplicated <- GS %>% 
  filter(Authors == "JF Muth") %>%
  select(c(Authors, Title, Year))
knitr::kable(GS_duplicated[1:5,], caption = "Duplicates")
```

In a nutshell, google scholar may be a powerful tool for exploring and collecting data on older periods but it is important to keep in mind that he came with a price on the quality of data. Working with google scholar implies a data cleaning burden. Together with the fact that google does not offer a data export service, google scholar became less and less useful as your work with larger data. Google scholar remains, however, a useful exploratory tool, and might be used as complementary sources for research questions that involve working with small amounts of data. 



